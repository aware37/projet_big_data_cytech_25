# Projet Big Data Cytech 25
Pour l'instant voici les exercices finis :
- ex01_data_retrieval
- ex02_data_ingestion
- ex03_sql_table_creation
- ex04_dashboard

## Pr√©requis

- Docker et Docker Compose install√©s
- SBT install√©
- Python 3.8+
- `uv` install√© ([Installation](https://github.com/astral-sh/uv#installation))

  Si `uv` n'est pas install√©, installe-le avec :
  ```sh
  curl -LsSf https://astral.sh/uv/install.sh | sh
  ```

---

## 0. Initialiser l'environnement Python

√Ä la racine du projet, initialise `uv` :

```sh
uv init
uv sync
```

Cela cr√©e `.venv` et installe les d√©pendances d√©finies dans `pyproject.toml`.

### Ajouter des d√©pendances

Ajouter de nouvelle d√©pendance Python dans `pyproject.toml` :

```toml
[project]
dependencies = [
    "streamlit",
    "pandas",
    "psycopg2-binary",
    "matplotlib",
    "nouvelle-dependance>=1.0.0",
]
```

Puis synchronise :

```sh
uv sync
```

---

## 1. Lancer l'infrastructure

√Ä la racine du projet, lance :

```sh
sudo docker-compose up -d
```

Cela d√©marre :
- MinIO (stockage)
- PostgreSQL (base de donn√©es)
- Spark

---

## 2. Configurer MinIO

1. Ouvre [http://localhost:9000](http://localhost:9000) dans ton navigateur.
2. Connecte-toi avec :
   - **Identifiant** : `minio`
   - **Mot de passe** : `minio123`
3. Cr√©e un bucket nomm√© :  
   ```
   nyc-yellow-tripdata
   ```

---

## 3. T√©l√©charger et envoyer les donn√©es sur MinIO

Dans le dossier `ex01_data_retrieval` :

```sh
cd ex01_data_retrieval
sbt run
```

Cela t√©l√©charge le fichier Parquet et l'upload automatiquement dans le bucket MinIO.

---

## 4. V√©rifier la base de donn√©es PostgreSQL

Reviens √† la racine du projet, puis connecte-toi √† la base :

```sh
sudo docker exec -it postgres psql -U postgres -d bigdata_db
```

### Tester les tables de dimension

Ex√©cute les requ√™tes suivantes dans le client `psql` :

```sql
-- Nombre d'emplacements import√©s
SELECT count(*) FROM dim_location;

-- Afficher quelques emplacements
SELECT * FROM dim_location LIMIT 5;

-- Afficher quelques vendeurs
SELECT * FROM dim_vendor LIMIT 5;

-- Afficher quelques types de paiement
SELECT * FROM dim_payment_type LIMIT 5;

-- Afficher quelques codes tarifaires
SELECT * FROM dim_rate_code LIMIT 5;
```

Pour quitter `psql` :

```
\q
```

---

## 5. Ing√©rer les donn√©es nettoy√©es dans PostgreSQL

Dans le dossier `ex02_data_ingestion` :

```sh
cd ex02_data_ingestion
sbt run
```

Cela :
- Lit les donn√©es brutes depuis MinIO
- Nettoie les donn√©es (Branche 1)
- Les sauvegarde en Parquet nettoy√© dans MinIO (Branche 1)
- Les ins√®re dans la table `fact_trips` de PostgreSQL (Branche 2)

### V√©rifier l'insertion des donn√©es

Reviens √† la racine et connecte-toi √† la base :

```sh
cd ..
sudo docker exec -it postgres psql -U postgres -d bigdata_db
```

Puis v√©rifie que les donn√©es ont bien √©t√© ins√©r√©es :

```sql
-- Nombre de trajets ins√©r√©s
SELECT COUNT(*) FROM fact_trips;

-- Afficher quelques trajets
SELECT * FROM fact_trips LIMIT 5;

-- Statistiques par vendor
SELECT v.vendor_name, COUNT(*) as nb_trajets, SUM(f.total_amount) as revenue
FROM fact_trips f
JOIN dim_vendor v ON f.vendor_id = v.vendor_id
GROUP BY v.vendor_name;
```

Pour quitter `psql` :

```
\q
```

---

## 6. Lancer le Dashboard Streamlit

Lance le dashboard depuis la racine du projet :

```sh
cd ex04_dashboard
uv run streamlit run app.py
```

Le dashboard s'ouvre automatiquement sur [http://localhost:8501](http://localhost:8501) üéâ

---

**R√©sum√© des commandes principales** :

```sh
# 0. Initialiser l'environnement Python (une seule fois)
uv init
uv sync

# 1. Lancer l'infrastructure
sudo docker-compose up -d

# 2. Configurer MinIO (via interface web http://localhost:9000)

# 3. T√©l√©charger les donn√©es
cd ex01_data_retrieval
sbt run
cd ..

# 4. V√©rifier PostgreSQL
sudo docker exec -it postgres psql -U postgres -d bigdata_db
# (puis requ√™tes SQL ci-dessus)

# 5. Ins√©rer les donn√©es nettoy√©es
cd ex02_data_ingestion
sbt run
cd ..

# 6. V√©rifier l'insertion
sudo docker exec -it postgres psql -U postgres -d bigdata_db
# (puis requ√™tes SQL ci-dessus)

# 7. Lancer le dashboard
cd ex04_dashboard
uv run streamlit run app.py
```

---

## Code minimal pour Spark + MinIO

```scala
import org.apache.spark.sql.{SparkSession, DataFrame}

object SparkApp extends App {
  val spark = SparkSession.builder()
    .appName("SparkApp")
    .master("local")
    .config("spark.hadoop.fs.s3a.access.key", "minio")
    .config("spark.hadoop.fs.s3a.secret.key", "minio123")
    .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000/")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false")
    .config("spark.hadoop.fs.s3a.attempts.maximum", "1")
    .config("spark.hadoop.fs.s3a.connection.establish.timeout", "6000")
    .config("spark.hadoop.fs.s3a.connection.timeout", "5000")
    .getOrCreate()
  spark.sparkContext.setLogLevel("WARN")
}
```

---

## Nettoyage et D√©pannage

### Vider MinIO (si stockage plein)

```sh
sudo docker exec -it minio mc rb --force minio/nyc-yellow-tripdata
```

### Red√©marrer la BDD PostgreSQL

```sh
sudo docker-compose down postgres
sudo docker-compose up -d postgres
sleep 10
```

### Tout recommencer

```sh
sudo docker-compose down
sudo docker volume prune
sudo docker-compose up -d
```

---

## Modalit√©s de rendu

1. Pull Request vers la branch `master`
2. D√©pot du rapport et du code source zipp√© dans cours.cyu.fr (Les acc√®s seront bient√¥t ouverts)

Date limite de rendu : 7 f√©vrier 2026